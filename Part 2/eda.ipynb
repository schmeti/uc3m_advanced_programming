{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of Employee Attrition/Burnout\n",
    "Authors: Florencia Luque and Simon Schmetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available = pd.read_csv(\"attrition_availabledata_10.csv.gz\", compression='gzip')\n",
    "data_competetion = pd.read_csv(\"attrition_competition_10.csv.gz\", compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2940 instances and 31 variables including the attrition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available.isna().sum()/data_available.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no Nan values in any of the columns in the available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is unbalanced with a 83.8% of No and 16.12% of Yes in the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available.groupby(\"Attrition\").count()/data_available.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = ['JobInvolvement', 'PerformanceRating','EnvironmentSatisfaction', 'JobSatisfaction',\n",
    "                          'WorkLifeBalance','BusinessTravel', 'Department', 'Education','EducationField',\n",
    "                          'EmployeeID','Gender', 'JobLevel','JobRole','MaritalStatus','StockOptionLevel','Attrition']\n",
    "numeric_variables = ['hrs','absences','Age','DistanceFromHome','MonthlyIncome','NumCompaniesWorked',\n",
    "                     'PercentSalaryHike', 'StandardHours','TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany',\n",
    "       'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
    "\n",
    "constant_variables = [\"EmployeeCount\",'Over18','StandarHours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_available[categorical_variables].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff from Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Outer evaluation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42)\n",
    "\n",
    "# Inner evaluation with 3-fold CV\n",
    "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Store inner evaluation scores\n",
    "inner_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Tree with default parameters\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_default_scores = cross_val_score(tree_reg, X_train, y_train, cv=inner, scoring='neg_root_mean_squared_error')\n",
    "inner_scores['Tree Default'] = -tree_default_scores.mean()\n",
    "\n",
    "# Regression Tree with hyperparameter tuning\n",
    "param_grid_tree = {'max_depth': [10, 20, 30],\n",
    "                   'min_samples_split': [2, 10, 20]}\n",
    "grid_search_tree = GridSearchCV(tree_reg, param_grid_tree, cv=inner, scoring='neg_root_mean_squared_error')\n",
    "grid_search_tree.fit(X_train, y_train)\n",
    "inner_scores['Tree Tuned'] = -grid_search_tree.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN with StandardScaler using Pipeline:\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "knn_std_default_scores = cross_val_score(knn_pipeline, X_train, y_train, cv=inner, scoring='neg_root_mean_squared_error')\n",
    "inner_scores['KNN Standard Default'] = -knn_std_default_scores.mean()\n",
    "\n",
    "# KNN with HPO:\n",
    "pipe_param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7],\n",
    "    'knn__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(knn_pipeline, pipe_param_grid, cv=inner, scoring='neg_root_mean_squared_error')\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "inner_scores['KNN Standard Tuned'] = -grid_search_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trees with default hyper-parameters:\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "empty_param_grid_tree = {}\n",
    "\n",
    "grid_search_tree_default = GridSearchCV(tree_reg, empty_param_grid_tree, cv=inner, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid_search_tree_default.fit(X_train, y_train)\n",
    "\n",
    "inner_scores['Tree Default'] = -grid_search_tree_default.best_score_\n",
    "\n",
    "#Trees with HPO:\n",
    "\n",
    "param_grid_tree = {'max_depth': [10, 20, 30],\n",
    "                   'min_samples_split': [2, 10, 20]}\n",
    "\n",
    "grid_search_tree = GridSearchCV(tree_reg, param_grid_tree, cv=inner, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid_search_tree.fit(X_train, y_train)\n",
    "\n",
    "inner_scores['Tree Tuned'] = -grid_search_tree.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Regressor using mean\n",
    "dummy_reg = DummyRegressor(strategy='mean')\n",
    "dummy_scores = cross_val_score(dummy_reg, X_train, y_train, cv=inner, scoring='neg_root_mean_squared_error')\n",
    "inner_scores['Dummy Mean'] = -dummy_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print inner evaluation scores and ratios\n",
    "print(f\"{'Model': <21} {'Inner RMSE': <15} {'Model/Dummy RMSE Ratio': <20}\")\n",
    "\n",
    "for model, score in inner_scores.items():\n",
    "    ratio = score / inner_scores['Dummy Mean']\n",
    "    print(f\"{model: <21} {score: <15.4f} {ratio: <20.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer evaluation for best model (estimation of future performance):\n",
    "test_predictions = grid_search_knn.predict(X_test)\n",
    "test_rmse = root_mean_squared_error(y_test, test_predictions)\n",
    "print(f'\\nBest Model: KNN Standard Tuned')\n",
    "print(f'Best Model Test RMSE: {test_rmse:.4f}')\n",
    "\n",
    "# Outer evaluation for Dummy Regressor:\n",
    "\n",
    "dummy_reg.fit(X_train, y_train)\n",
    "dummy_predictions = dummy_reg.predict(X_test)\n",
    "dummy_outer_rmse = root_mean_squared_error(y_test, dummy_predictions)\n",
    "print(f'\\nDummy Outer RMSE: {dummy_outer_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "\n",
    "final_model = grid_search_knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation + Scaling for KNN and SVM\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Define steps in the pipeline\n",
    "knn = KNeighborsRegressor()\n",
    "scaler = StandardScaler()\n",
    "imputer = SimpleImputer(strategy='mean')  # Imputation transformer for completing missing values with the mean\n",
    "\n",
    "# Update the pipeline to include the imputation step\n",
    "classif = Pipeline([\n",
    "    ('imputation', imputer),\n",
    "    ('standardization', scaler),\n",
    "    ('knn', knn)\n",
    "])\n",
    "\n",
    "# Now you can fit the pipeline to your data and make predictions\n",
    "classif.fit(X_train, y_train)\n",
    "y_hat = classif.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline for preprocessing data with categorical ordinal, categorical non-ordinal, and numerical features\n",
    "\n",
    "# Define columns by type\n",
    "numerical_features = ['age', 'income', 'years_experience']\n",
    "ordinal_features = ['education_level']\n",
    "non_ordinal_features = ['city', 'gender']\n",
    "\n",
    "# Define transformations for each type\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))  # Impute missing\n",
    "])\n",
    "\n",
    "ordinal_transformer = OrdinalEncoder(categories=[['High School', 'Bachelor\\'s', 'Master\\'s', 'PhD']])\n",
    "\n",
    "non_ordinal_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine preprocessing into a column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_transformer, numerical_features),  # Impute numerical features\n",
    "    ('ord', ordinal_transformer, ordinal_features),  # Encode ordinal categorical features\n",
    "    ('non_ord', non_ordinal_transformer, non_ordinal_features)  # One-hot encode non-ordinal categorical features\n",
    "])\n",
    "\n",
    "# Define the full pipeline with scaling applied to all features\n",
    "knn_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocess all features\n",
    "    ('scaler', StandardScaler()),  # Scale all features after preprocessing\n",
    "    ('knn', KNeighborsRegressor())  # Apply KNN\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
